{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The tranformer architecture\n",
    "\n",
    "The transformer architecture describes an Encoder (left) and Decoder (right).\n",
    "\n",
    "<img src=\"assets/transformer.png\" alt=\"Image\" style=\"width:20%; display: block; margin: 0 auto;\"/>\n",
    "\n",
    "## Encoder\n",
    "- Encodes words or word-tokens to vectors. One input word/token results in one output word/token vector.\n",
    "- All words are simulatniously passed into the encoder. This has several advantages e.g., usage of parallelization on GPUs and better captioning of word meaning through the attention block.\n",
    "\n",
    "#### Encoder Blocks\n",
    "- Input Embedding: Some vector embedding of each word.\n",
    "- Positional Encoding: Encoding of each word with respect to the position of the word in a sentence.\n",
    "- Input Embedding and Positional Encoding forms the Input to the Encoder Block.\n",
    "- Multi-Head Attention: Every single word has 3 vectors: Q, K and V. More explanation below. \"Multi\" because we can stack the results on top of each other to get multi-attention. \n",
    "    - Q = What I am looking for [sequence length x d<sub>k</sub>].\n",
    "    - K = What I can offer [sequence length x d<sub>k</sub>]\n",
    "    - V: What I actually offer [sequence length x d<sub>v</sub>]\n",
    "- Feed Forward:\n",
    "- Add & Norm:\n",
    "\n",
    "## Decoder\n",
    "- The decoder first takes in the output of the encoder and a start token (<START>).\n",
    "- Then it will start generating the first word e.g. a translated word.\n",
    "- The translated word is then taken as an input for the decoder to generate the next word until the end of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention (Single Attention Head Logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length, d_k, d_v = 4, 8, 8\n",
    "q = np.random.randn(sequence_length, d_k) # creates 8 x 1 vectors for each word in the sequence\n",
    "k = np.random.randn(sequence_length, d_k) # creates 8 x 1 vectors for each word in the sequence\n",
    "v = np.random.randn(sequence_length, d_v) # creates 8 x 1 vectors for each word in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q: [[ 0.68990754  1.04859215  0.23305695  2.12474519 -0.93268337  1.18915204\n",
      "   0.58470948  0.16119415]\n",
      " [-1.44818151 -0.98405672 -0.32402655  0.35952454  0.51441908 -0.32262288\n",
      "  -2.80584223 -0.39850066]\n",
      " [ 1.06331365  0.87722272 -1.18467051 -1.1764606  -0.33807337 -0.99574534\n",
      "  -0.06448032  1.63173451]\n",
      " [ 0.85681042  0.26314427  0.29461072  0.2640756   0.38752057 -0.71687163\n",
      "   0.40375062  0.39431383]]\n",
      "k: [[ 0.26796196 -0.39363637  0.63026818 -1.71877323  0.19935174 -0.34672076\n",
      "   0.01065149 -0.25848164]\n",
      " [ 0.35681432 -0.35027568  0.97410492  0.18258534  0.86790105  0.80261015\n",
      "  -0.35439264 -1.74457811]\n",
      " [ 1.17133802 -1.74320744 -1.84310561  1.14770055 -1.07169149  0.70438718\n",
      "   1.43048931  0.54013891]\n",
      " [-0.83789485 -0.06584253 -0.22458307  0.88494639 -2.44562938 -0.96672727\n",
      "   0.39516584 -0.94748545]]\n",
      "v: [[-0.28318036 -0.73355815 -1.17730055 -0.22004776  0.62210573  0.94864427\n",
      "   0.22660927  1.90664358]\n",
      " [-1.1770797   0.84414267  0.39552354 -0.80853454 -0.57735204  0.1998187\n",
      "   1.51302064  0.69892223]\n",
      " [ 0.41453796 -1.39648173 -0.45725561  0.5415451   0.18099714 -2.07086988\n",
      "   0.78158265  0.75917078]\n",
      " [ 1.27936768 -1.10066096  0.01066041 -1.21603716 -0.71495532 -0.50645489\n",
      "   0.50318535 -0.57329572]]\n"
     ]
    }
   ],
   "source": [
    "print(\"q:\", q)\n",
    "print(\"k:\", k)  \n",
    "print(\"v:\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention\n",
    "\n",
    "<img src=\"assets/attention.png\" alt=\"Image\" style=\"width:20%; display: block; margin: 0 auto;\"/>\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Self-Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.36663526,  0.15035791,  3.74988412,  2.39057327],\n",
       "       [-0.53533274,  1.45507711, -3.97857637, -0.00824205],\n",
       "       [ 1.07041954, -5.2131086 ,  0.99961191, -1.50586001],\n",
       "       [ 0.08599198, -0.52128918,  0.17526811, -1.03648547]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an initial attention matrix, we need every word to look at every word to look at every other word \n",
    "np.matmul(q, k.T) # 4 x 4 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5438387 ,  0.05315955,  1.32578425,  0.84519529],\n",
       "       [-0.1892687 ,  0.51444745, -1.40663917, -0.00291401],\n",
       "       [ 0.37845046, -1.84311222,  0.35341618, -0.53240191],\n",
       "       [ 0.03040276, -0.18430356,  0.06196663, -0.36645295]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k) # scale by square root of d_k\n",
    "scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "- This is to ensure words don't get context from words generated in the future\n",
    "- Not requred in the encoders, but reqiured in the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((sequence_length, sequence_length)))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1.0] = 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5438387 ,        -inf,        -inf,        -inf],\n",
       "       [-0.1892687 ,  0.51444745,        -inf,        -inf],\n",
       "       [ 0.37845046, -1.84311222,  0.35341618,        -inf],\n",
       "       [ 0.03040276, -0.18430356,  0.06196663, -0.36645295]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "- Is used to convert a vector into a probability distribution\n",
    "- Advantage: Values add up to one and are more interpretable\n",
    "\n",
    "$$\n",
    "\\text{softmax}(x) = \\frac{e^{x_i}}{\\sum_j e_j^x}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [3.8750943 , 0.66901118, 0.        , 0.        ],\n",
       "       [6.83659061, 0.06332252, 0.46804674, 0.        ],\n",
       "       [4.82707869, 0.33263632, 0.34971503, 0.19150614]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28318036, -0.73355815, -1.17730055, -0.22004776,  0.62210573,\n",
       "         0.94864427,  0.22660927,  1.90664358],\n",
       "       [-1.88483005, -2.27786612, -4.29754097, -1.39362448,  2.02446342,\n",
       "         3.80976695,  1.89036001,  7.85601045],\n",
       "       [-1.81650067, -5.61520223, -8.23769332, -1.30210651,  4.30123796,\n",
       "         5.52888165,  2.01086031, 13.43452653],\n",
       "       [-1.3684964 , -3.95930437, -5.70922451, -1.37462795,  2.73728416,\n",
       "         3.82444397,  1.96684068,  9.59170929]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiplie attention matrix with value matrix\n",
    "new_v = np.matmul(attention, v)\n",
    "new_v # new matrix which should encapsulate the context of a word better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.28318036, -0.73355815, -1.17730055, -0.22004776,  0.62210573,\n",
       "         0.94864427,  0.22660927,  1.90664358],\n",
       "       [-1.1770797 ,  0.84414267,  0.39552354, -0.80853454, -0.57735204,\n",
       "         0.1998187 ,  1.51302064,  0.69892223],\n",
       "       [ 0.41453796, -1.39648173, -0.45725561,  0.5415451 ,  0.18099714,\n",
       "        -2.07086988,  0.78158265,  0.75917078],\n",
       "       [ 1.27936768, -1.10066096,  0.01066041, -1.21603716, -0.71495532,\n",
       "        -0.50645489,  0.50318535, -0.57329572]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
